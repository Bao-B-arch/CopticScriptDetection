---
title: "Coptic OCR"
lang: fr
author: "Antenainaskara"
echo: False
format: 
    pdf:
        toc: true
        toc-depth: 2
        toc-title: Table des Matières
        number-sections: true
        colorlinks: true
        default-image-extension: tex
        fig-pos: 'h'
    revealjs:
        toc-depth: 2
        toc-title: Table des Matières
        slide-level: 3
    docx:
        toc: true
        toc-depth: 2
        toc-title: Table des Matières
        number-sections: true
        colorlinks: true
        default-image-extension: tex
        fig-pos: 'h'
jupyter: python3
---

::: {.content-visible when-format="pdf"}

\newpage

:::

## Objectif du projet

L'objectif du projet est de construire un modèle statistique permettant d'effectuer la reconnaissance optique de caractères de l'alphabet copte. On appelle ce procédé l'OCR pour *optical character recognition* en anglais.

Pour remplir cet objectif, j'ai choisi par simplicité d'utiliser une technique se basant uniquement sur du Machine Learning. C'est à dire en calculant des features spécifiques et en effectuant l'apprentissage de deux modèles différents, un `SVM` (Support Machine Vector) et un `Random Forest`, aussi appelé dans la suite `RFC`.

### Cadrage du projet

Il s'agit d'un projet de classification en contexte supervisé. En effet, on veut à partir d'une image d'un caractère copte, calculer un vecteur de features et l'utiliser pour prédire la lettre copte associée à l'image. Ici, on utilisera le terme de classes pour parler de lettres coptes.

### Plan du rapport

Ce rapport sera construit de manière chronologique, c'est-à-dire en présentant les étapes successives permettant de construire ces modèles (`SVM` et `RFC`). 

Aussi, on commencera par une description de la base de données, puis des features calculées, ensuite on présentera l'ensemble des transformations appliquées à nos données, puis des stratégies utilisées pour réduire la quantité de features, puis de l'entrainement des modèles et la recherche d'hyperparamètres et pour finir on parlera des perfomances des divers modèles générés.

::: {.content-visible when-format="pdf"}

\newpage

:::

```{python}
import yaml

with open(f"report/OCR_coptic_16_l1_report.yaml", "r") as file:
    data = yaml.safe_load(file)

data = data["states"]
nb_letters = data["initial_data"]["shape"][0]
```

## Description de la base de données

La base de données se compose de `{python} nb_letters` images de taille `28x28` en niveau de gris. Ainsi chaque pixel de l'image prend une valeur comprise en `0` pour du noir et `255` pour du blanc.

### Valeurs cibles

```{python}
dict_letter = data["after_letter_to_remove"]["metadata"]["LETTER_REMOVED"]

str_letter = ", ".join([f"`{l}`" for l in dict_letter])
images_letter = ", ".join([f"{l}" for l in dict_letter.values()])
```

::: {.content-visible when-format="pdf"}

On dénombre `{python} len(data["initial_data"]["metadata"]["TARGET"])` lettres copte dans notre base de données. Cependant `{python} str_letter` seront retirées car n'ayant pas assez d'images associées (respectivement `{python} images_letter`). La base de données se comporte donc de `{python} len(data["initial_data"]["metadata"]["TARGET"]) - len(dict_letter)` classes effectives et de `{python} data["after_letter_to_remove"]["shape"][0]` images, aussi appelées `samples` dans ce contexte.

:::

![distribution des lettres](./graph/graphs_16_l1/dataset.svg){#fig-split}

On peut déjà conclure que notre jeu de données n'est pas équilibré et que cela pourra engendrer des difficultés à détecter les lettres les moins fréquentes. On sait aussi que l'`accuracy`, la métrique par défaut utilisée en machine learning sera moines pertinente. On lui préférera un indicateur pouvant gérer le deséquilibre de classe comme le `coefficient de corrélation de Matthews`, qui sera noté `MCC` par la suite.

Une possibilité pour régler ce déséquilibre serait la génération de données artificielles. On pourrait, pour les lettres les moins présentes, générer des images similaires en introduisant des bruits gaussiens ou en les tournant légèrement.

::: {.content-visible when-format="pdf"}

\newpage

:::

## Description des features

Chaque image est découpée en `{python} data["initial_data"]["metadata"]["NB_PATCHES"]` patches chevauchant. Pour chaque patch, on calcule la moyenne et la variance des niveaux de gris donnant ainsi `{python} len(data["initial_data"]["metadata"]["FEATURES"])` features par image.

Chaque patch ayant une position sur l'image, on peut consistuer une nouvelle image à partir des moyennes des patches et une autre à partir des variances. Concernant les variances, celles-ci étant par nature homogène au carré de la moyenne, leur représentation directe comme image en niveau de gris n'aurait pas de sens. Aussi, on affiche à la place l'écart-type des niveaux de gris qui est suffisamment proche de la variance pour une représentation visuelle. Exemple pour une lettre `qima`:

::: {layout-ncol=3}

![lettre](./Images_Traitees/qima/objet_1_585.png){#fig-letter width=40%}

```{python}
npatches = data["initial_data"]["metadata"]["NB_PATCHES"]
```
![features moyennes](`{python} f"./export/{npatches}patches/qima/example_mean_qima.png"`){#fig-features width=40%}

![features "variance"](`{python} f"./export/{npatches}patches/qima/example_std_qima.png"`){#fig-features width=40%}

:::

Exemple pour une lettre `Fai`:

::: {layout-ncol=3}

![lettre](./Images_Traitees/Fai/objet_1_83.png){#fig-letter width=40%}

```{python}
npatches = data["initial_data"]["metadata"]["NB_PATCHES"]
```
![features moyennes](`{python} f"./export/{npatches}patches/Fai/example_mean_Fai.png"`){#fig-features width=40%}

![features "variance"](`{python} f"./export/{npatches}patches/Fai/example_std_Fai.png"`){#fig-features width=40%}

:::

```{python}
train_percent = f"{data['train_test_split']['metadata']['TRAIN_SIZE']['percent']:.2f}"
test_percent = f"{data['train_test_split']['metadata']['TEST_SIZE']['percent']:.2f}"
```

| Jeu de données | Nombre d'images |
|--|--|
| TOTAL | `{python} data["train_test_split"]["shape"][0]`
| TRAIN | `{python} data["train_test_split"]["metadata"]["TRAIN_SIZE"]["value"]` (`{python} train_percent`%)
| TEST | `{python} data["train_test_split"]["metadata"]["TEST_SIZE"]["value"]` (`{python} test_percent`%)


## Transformation des données

Afin d'éviter des problèmes numériques et des imprécisions lors de l'apprentissage, il est nécessaire de transformer les données pour les rapprocher de l'intervale `[-1, 1]`.

Il sera utilisé un Standard Scaler: $z = \frac{x - m}{s}$ avec $m$ la moyenne et $s$ l'écart-type.

![Analyse des outliers](./graph/graphs_16_None/scaling.svg){#fig-scaling}

::: {.content-visible when-format="revealjs"}

---

:::

::: {.content-visible when-format="pdf"}

\newpage

:::

```{python}
nb_outliers = data["initial_data"]["metadata"]["OUTLIERS_ANALYSIS"]["nb_outliers"]
pc_outliers = data["initial_data"]["metadata"]["OUTLIERS_ANALYSIS"]["percent"]

import pandas as pd
df_outlier = pd.DataFrame({"nb_outliers": nb_outliers, "percent": pc_outliers})
df_outlier = df_outlier.sort_values(by="nb_outliers", ascending=False)
df_outlier.index.name = "features"

table = df_outlier.head()
features_over = df_outlier.index[df_outlier.loc[:, "percent"] > 5.0]
```

Bien qu'il y ait `{python} len(features_over)` features dépassant les `5%` d'outliers, on considère que la transformation des données est valide.

```{python} 
table
```


## Recherche des hyper paramètres

On considère deux modèles, un SVM et un Random Forest. Actuellement on essaie d'optimiser les hyper paramètres uniquement du SVM. On étudie pour cela deux hyper paramètres `C` et `gamma`.

### C

`C` représente la marge d'erreur que l'on autorise. S'il est petit, alors on autorise des erreurs de classification. Au contraire, avec une grande valeur, on est plus strict mais avec un risque d'overfitting

### Gamma

`Gamma` représente la distance à laquelle un élément éloigné a une influence sur les vecteurs de support. Si `gamma` est faible alors les éléments éloigné auront une influence, si élevé, seuls les éléments proches auront une influence.

### Résultats

En utilisant un shuffle split stratifié, on obtient les paramètres optimaux suivants:

| C | Gamma |
|---|---|
|`{python} f"{data['model_search_svm']['metadata']['model_params']['C']:.2f}"` | `{python} f"{data['model_search_svm']['metadata']['model_params']['gamma']:.2f}"` |

## Résultats de l'entrainement

 Après apprentissage on trouve:

| Modèle | Accuracy | Matthews Correlation Coefficient |
|---|---|---|
|Random Forest | `{python} f"{data['eval_rfc']['metadata']['accuracy_score']:.2f}"` | `{python} f"{data['eval_rfc']['metadata']['matthews_corrcoef']:.2f}"` |
|SVM | `{python} f"{data['eval_search_svm']['metadata']['accuracy_score']:.2f}"` | `{python} f"{data['eval_search_svm']['metadata']['matthews_corrcoef']:.2f}"` |

### Matrices de confusion

::: {layout-ncol=2}

![RFC](./graph/graphs_16_l1/cm_eval_RFC.svg){#fig-cm_rfc}

![SVM](./graph/graphs_16_l1/cm_eval_search_svm.svg){#fig-cm_svm}

:::

## Exemple de prédictions

```{python}
pd.DataFrame(data["example"]["metadata"])
```